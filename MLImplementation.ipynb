{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jukGnBCbXK1"
      },
      "outputs": [],
      "source": [
        "! pip install -q pyspark==3.3.0 spark-nlp==4.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHDWXsb9bnMX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "a80d21a4-6c64-486e-d4e5-42798d9b41cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP version:  4.2.0\n",
            "Apache Spark version:  3.3.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f52984b4310>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://37c4bbeff790:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import sparknlp\n",
        "\n",
        "from sparknlp.base import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.annotator import *\n",
        "\n",
        "from pyspark.ml.feature import CountVectorizer, HashingTF, IDF, OneHotEncoder, StringIndexer, VectorAssembler, SQLTransformer\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "spark = sparknlp.start()#gpu=True\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiZxRtDOcJae",
        "outputId": "25079e21-78ad-450d-d8cd-89a56c1ca2fd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20800"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df = spark.read.format(\"csv\").option(\"header\", True).option(\"multiline\", True).option(\"escape\", \"\\\"\").load(\"train.csv\")\n",
        "\n",
        "# Reanme label column for using StringIndexer\n",
        "df = df.withColumnRenamed(\"label\",\"label_str\")\n",
        "\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x52UIGXkVwUL"
      },
      "source": [
        "# TF-IDF + LogReg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qFbm8sTcGLd"
      },
      "source": [
        "### Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AyJwEefcAOp",
        "outputId": "07fbbc2f-c89a-4394-9723-881537dbf83a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+--------------------+---------+\n",
            "| id|                text|               title|label_str|\n",
            "+---+--------------------+--------------------+---------+\n",
            "|  0|house dem aide: w...|house dem aide: w...|        1|\n",
            "|  1|ever get the feel...|flynn: hillary cl...|        0|\n",
            "|  2|why the truth mig...|why the truth mig...|        1|\n",
            "|  3|videos 15 civilia...|15 civilians kill...|        1|\n",
            "|  4|print \\nan irania...|iranian woman jai...|        1|\n",
            "|  5|in these trying t...|jackie mason: hol...|        0|\n",
            "|  6|ever wonder how b...|life: life of lux...|        1|\n",
            "|  7|paris  —   france...|benoît hamon wins...|        0|\n",
            "|  8|donald j. trump i...|excerpts from a d...|        0|\n",
            "|  9|a week before mic...|a back-channel pl...|        0|\n",
            "| 10|organizing for ac...|obama’s organizin...|        0|\n",
            "| 11|the bbc produced ...|bbc comedy sketch...|        0|\n",
            "| 12|the mystery surro...|russian researche...|        1|\n",
            "| 13|clinton campaign ...|us officials see ...|        1|\n",
            "| 14|yes, there are pa...|re: yes, there ar...|        1|\n",
            "| 15|guillermo barros ...|in major league s...|        0|\n",
            "| 16|the scandal engul...|wells fargo chief...|        0|\n",
            "| 17|a caddo nation tr...|anonymous donor p...|        1|\n",
            "| 18|fbi closes in on ...|fbi closes in on ...|        1|\n",
            "| 19|wednesday after  ...|chuck todd: ’buzz...|        0|\n",
            "+---+--------------------+--------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Expanding contractions and lowering\n",
        "from pyspark.sql.functions import lower, regexp_replace\n",
        "\n",
        "replacement_patterns = [\n",
        "  (r'won\\'t', 'will not'),\n",
        "  (r'can\\'t', 'cannot'),\n",
        "  (r'i\\'m', 'i am'),\n",
        "  (r'ain\\'t', 'is not'),\n",
        "  (r'(\\w+)\\'ll', '\\g<1> will'),\n",
        "  (r'(\\w+)n\\'t', '\\g<1> not'),\n",
        "  (r'(\\w+)\\'ve', '\\g<1> have'),\n",
        "  (r'(\\w+)\\'s', '\\g<1> is'),\n",
        "  (r'(\\w+)\\'re', '\\g<1> are'),\n",
        "  (r'(\\w+)\\'d', '\\g<1> would')\n",
        "]\n",
        "\n",
        "# create a copy\n",
        "df_clean = df.alias('df_clean')\n",
        "\n",
        "for (pattern, repl) in replacement_patterns:\n",
        "  df_clean = df_clean.select('id', \n",
        "          (lower(regexp_replace('text', pattern, repl)).alias('text')), \n",
        "          (lower(regexp_replace('title', pattern, repl)).alias('title')),\n",
        "          'label_str')\n",
        "\n",
        "df_clean.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oV7vePoPcIgw"
      },
      "outputs": [],
      "source": [
        "# Removing puntuation and digits\n",
        "df_clean = df_clean.select('id', \n",
        "                     (regexp_replace('text', \"[^a-z\\\\s]\", \"\").alias('text')),\n",
        "                     (regexp_replace('title', \"[^a-z\\\\s]\", \"\").alias('title')),\n",
        "                     'label_str')\n",
        "\n",
        "# Removing extra spaces\n",
        "df_clean = df_clean.select('id', \n",
        "                     (regexp_replace('text', r'\\s+', \" \").alias('text')),\n",
        "                     (regexp_replace('title', r'\\s+', \" \").alias('title')),\n",
        "                     'label_str')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WmbiBeBPgKe",
        "outputId": "ee6a8871-7c7d-4f20-eeff-53499250ce1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Count: 14584\n",
            "Test Dataset Count: 6216\n"
          ]
        }
      ],
      "source": [
        "(trainingData, testData) = df_clean.randomSplit([0.7, 0.3], seed = 100)\n",
        "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
        "print(\"Test Dataset Count: \" + str(testData.count()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuNYEIIFG387"
      },
      "source": [
        "### Titles "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8Ek98E0b00a",
        "outputId": "556300da-3edf-47c5-c913-0f0b75039732"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.39 s, sys: 177 ms, total: 1.57 s\n",
            "Wall time: 3min 21s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "      .setInputCol(\"title\") \\\n",
        "      .setOutputCol(\"document\")\n",
        "    \n",
        "tokenizer = Tokenizer() \\\n",
        "      .setInputCols([\"document\"]) \\\n",
        "      .setOutputCol(\"token\")\n",
        "      \n",
        "normalizer = Normalizer() \\\n",
        "      .setInputCols([\"token\"]) \\\n",
        "      .setOutputCol(\"normalized\")\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "      .setInputCols(\"normalized\")\\\n",
        "      .setOutputCol(\"cleanTokens\")\\\n",
        "      .setCaseSensitive(False)\n",
        "\n",
        "finisher = Finisher() \\\n",
        "      .setInputCols([\"cleanTokens\"]) \\\n",
        "      .setOutputCols([\"token_features\"]) \\\n",
        "      .setOutputAsArray(True) \\\n",
        "      .setCleanAnnotations(False)\n",
        "\n",
        "\n",
        "label_stringIdx = StringIndexer(inputCol = \"label_str\", outputCol = \"label\")\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"token_features\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) \n",
        "\n",
        "nlp_pipeline = Pipeline(\n",
        "    stages=[document_assembler, \n",
        "            tokenizer,\n",
        "            normalizer,\n",
        "            stopwords_cleaner, \n",
        "            finisher,\n",
        "            hashingTF,\n",
        "            idf,\n",
        "            label_stringIdx])\n",
        "\n",
        "nlp_model = nlp_pipeline.fit(trainingData)\n",
        "\n",
        "trainingData_processed = nlp_model.transform(trainingData)\n",
        "testData_processed = nlp_model.transform(testData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRP3T8htyV2A"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0)\n",
        "\n",
        "lrModel_tf = lr.fit(trainingData_processed)\n",
        "\n",
        "predictions_tf = lrModel_tf.transform(testData_processed)\n",
        "\n",
        "result = predictions_tf.select('title', 'label', 'prediction').toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wQmYIeRD7X4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a8a1dcd-858e-4c23-fd16-b4244991a170"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.89      0.96      0.93      3057\n",
            "         1.0       0.96      0.89      0.92      3159\n",
            "\n",
            "    accuracy                           0.92      6216\n",
            "   macro avg       0.93      0.92      0.92      6216\n",
            "weighted avg       0.93      0.92      0.92      6216\n",
            "\n",
            "0.924066924066924\n"
          ]
        }
      ],
      "source": [
        "# Compute metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "print(classification_report(result.label, result.prediction))\n",
        "print(accuracy_score(result.label, result.prediction))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.to_csv('tfidf_titles.csv')"
      ],
      "metadata": {
        "id": "nk1H2Cfwl7Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUCPyUzNG-wQ"
      },
      "source": [
        "### Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5ayaUdsG0ir",
        "outputId": "ac65896a-f7ff-4fde-c05f-f6d625f4002f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.99 s, sys: 581 ms, total: 4.57 s\n",
            "Wall time: 11min 3s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "      .setInputCol(\"text\") \\\n",
        "      .setOutputCol(\"document\")\n",
        "    \n",
        "tokenizer = Tokenizer() \\\n",
        "      .setInputCols([\"document\"]) \\\n",
        "      .setOutputCol(\"token\")\n",
        "      \n",
        "normalizer = Normalizer() \\\n",
        "      .setInputCols([\"token\"]) \\\n",
        "      .setOutputCol(\"normalized\")\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "      .setInputCols(\"normalized\")\\\n",
        "      .setOutputCol(\"cleanTokens\")\\\n",
        "      .setCaseSensitive(False)\n",
        "\n",
        "finisher = Finisher() \\\n",
        "      .setInputCols([\"cleanTokens\"]) \\\n",
        "      .setOutputCols([\"token_features\"]) \\\n",
        "      .setOutputAsArray(True) \\\n",
        "      .setCleanAnnotations(False)\n",
        "\n",
        "label_stringIdx = StringIndexer(inputCol = \"label_str\", outputCol = \"label\")\n",
        "\n",
        "hashingTF = HashingTF(inputCol=\"token_features\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
        "\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
        "\n",
        "nlp_pipeline = Pipeline(\n",
        "    stages=[document_assembler, \n",
        "            tokenizer,\n",
        "            normalizer,\n",
        "            stopwords_cleaner, \n",
        "            finisher,\n",
        "            hashingTF,\n",
        "            idf,\n",
        "            label_stringIdx])\n",
        "\n",
        "nlp_model = nlp_pipeline.fit(trainingData)\n",
        "\n",
        "trainingData_processed = nlp_model.transform(trainingData)\n",
        "testData_processed = nlp_model.transform(testData)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jr4a-lwtHSCS"
      },
      "outputs": [],
      "source": [
        "lrModel_tf = lr.fit(trainingData_processed)\n",
        "\n",
        "predictions_tf = lrModel_tf.transform(testData_processed)\n",
        "\n",
        "results = predictions_tf.select('text', 'label', 'prediction').toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KoV8jLvHW7x",
        "outputId": "3132c940-50ef-4451-e80d-6b97883b49a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.96      0.91      3057\n",
            "         1.0       0.96      0.86      0.91      3159\n",
            "\n",
            "    accuracy                           0.91      6216\n",
            "   macro avg       0.91      0.91      0.91      6216\n",
            "weighted avg       0.92      0.91      0.91      6216\n",
            "\n",
            "0.911036036036036\n"
          ]
        }
      ],
      "source": [
        "# Compute metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "print(classification_report(results.label, results.prediction))\n",
        "print(accuracy_score(results.label, results.prediction))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.to_csv('tfidf_text.csv')"
      ],
      "metadata": {
        "id": "7y5sdP71dypH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufrVen62ReOE"
      },
      "source": [
        "# BERT + SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSIkAP8wWmzL",
        "outputId": "906058fa-874f-4769-dbc8-80ed10fe9ca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Count: 14584\n",
            "Test Dataset Count: 6216\n"
          ]
        }
      ],
      "source": [
        "# Using raw text (without cleaning part)\n",
        "(trainingData, testData) = df.randomSplit([0.7, 0.3], seed = 100)\n",
        "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
        "print(\"Test Dataset Count: \" + str(testData.count()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFU5eqwwpQib"
      },
      "source": [
        "### Titles"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "document_assembler = DocumentAssembler()\\\n",
        "  .setInputCol(\"title\")\\\n",
        "  .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer().setInputCols([\"document\"])\\\n",
        "  .setOutputCol(\"token\")\n",
        " \n",
        "word_embeddings = BertEmbeddings.pretrained('bert_base_cased', 'en')\\\n",
        "  .setInputCols([\"document\", \"token\"])\\\n",
        "  .setOutputCol(\"embeddings\")\n",
        "\n",
        "label_stringIdx = StringIndexer(inputCol = \"label_str\", outputCol = \"label\")\n",
        "\n",
        "bert_pipeline = Pipeline().setStages(\n",
        "  [\n",
        "    document_assembler,\n",
        "    tokenizer,\n",
        "    word_embeddings,\n",
        "   label_stringIdx\n",
        "  ]\n",
        ")\n",
        "\n",
        "bert_model = bert_pipeline.fit(trainingData)\n",
        "trainingData_processed = bert_model.transform(trainingData)\n",
        "testData_processed = bert_model.transform(testData)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSqBkU7wGtab",
        "outputId": "52c6750c-5fa1-43ba-c171-374589e4773c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert_base_cased download started this may take some time.\n",
            "Approximate size to download 389.1 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "import pyspark.sql.types as T\n",
        "\n",
        "\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "\n",
        "#Average pooling -> sentence embeddings\n",
        "def avg_vectors(bert_vectors):\n",
        "  length = 768\n",
        "  avg_vec = [0] * length\n",
        "  for vec in bert_vectors:\n",
        "    for i, x in enumerate(vec[\"embeddings\"]):\n",
        "      avg_vec[i] += x\n",
        "    avg_vec[i] = avg_vec[i] / length\n",
        "  return avg_vec\n",
        "\n",
        "#create a udf\n",
        "avg_vectors_udf = F.udf(avg_vectors, T.ArrayType(T.DoubleType()))\n",
        "df_doc_vec_train = trainingData_processed.withColumn(\"doc_vector\", avg_vectors_udf(F.col(\"embeddings\")))\n",
        "df_doc_vec_test = testData_processed.withColumn(\"doc_vector\", avg_vectors_udf(F.col(\"embeddings\")))\n",
        "\n",
        "\n",
        "def dense_vector(vec):\n",
        "\treturn Vectors.dense(vec)\n",
        "\n",
        "dense_vector_udf = F.udf(dense_vector, VectorUDT())\n",
        "training = df_doc_vec_train.withColumn(\"features\", dense_vector_udf(F.col(\"doc_vector\")))\n",
        "test = df_doc_vec_test.withColumn(\"features\", dense_vector_udf(F.col(\"doc_vector\")))\n",
        "\n",
        "sgd = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
        "sgdModelBert = sgd.fit(training)\n",
        "\n",
        "predictions_bert = sgdModelBert.transform(test)"
      ],
      "metadata": {
        "id": "gmp5_onNG3Z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "results = predictions_bert.select('title', 'label', 'prediction').toPandas()\n",
        "\n",
        "print(classification_report(results.label, results.prediction))\n",
        "print(accuracy_score(results.label, results.prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsmKTr76fTfU",
        "outputId": "49f25e44-da8b-45f6-d671-f67f5656f5e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.83      0.93      0.88      3057\n",
            "         1.0       0.93      0.81      0.87      3159\n",
            "\n",
            "    accuracy                           0.87      6216\n",
            "   macro avg       0.88      0.87      0.87      6216\n",
            "weighted avg       0.88      0.87      0.87      6216\n",
            "\n",
            "0.8716216216216216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.to_csv('bert_titles.csv')"
      ],
      "metadata": {
        "id": "gFjoY1n6ls0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text"
      ],
      "metadata": {
        "id": "Av7_31QDkK-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_assembler = DocumentAssembler()\\\n",
        "  .setInputCol(\"text\")\\\n",
        "  .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer().setInputCols([\"document\"])\\\n",
        "  .setOutputCol(\"token\")\n",
        " \n",
        "word_embeddings = BertEmbeddings.pretrained('bert_base_cased', 'en')\\\n",
        "  .setInputCols([\"document\", \"token\"])\\\n",
        "  .setOutputCol(\"embeddings\")\n",
        "\n",
        "label_stringIdx = StringIndexer(inputCol = \"label_str\", outputCol = \"label\")\n",
        "\n",
        "bert_pipeline = Pipeline().setStages(\n",
        "  [\n",
        "    document_assembler,\n",
        "    tokenizer,\n",
        "    word_embeddings,\n",
        "   label_stringIdx\n",
        "  ]\n",
        ")\n",
        "\n",
        "\n",
        "bert_model = bert_pipeline.fit(trainingData)\n",
        "trainingData_processed = bert_model.transform(trainingData)\n",
        "testData_processed = bert_model.transform(testData)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9CtgO4kkRf6",
        "outputId": "dbf9f66a-b0f2-4adf-d5e6-541a36dd16fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert_base_cased download started this may take some time.\n",
            "Approximate size to download 389.1 MB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "import pyspark.sql.types as T\n",
        "\n",
        "from pyspark.ml.classification import LogisticRegression, GBTClassifier\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "\n",
        "def avg_vectors(bert_vectors):\n",
        "  length = 768\n",
        "  avg_vec = [0] * length\n",
        "  for vec in bert_vectors:\n",
        "    for i, x in enumerate(vec[\"embeddings\"]):\n",
        "      avg_vec[i] += x\n",
        "    avg_vec[i] = avg_vec[i] / length\n",
        "  return avg_vec\n",
        "\n",
        "\n",
        "#create a udf\n",
        "avg_vectors_udf = F.udf(avg_vectors, T.ArrayType(T.DoubleType()))\n",
        "df_doc_vec_train = trainingData_processed.withColumn(\"doc_vector\", avg_vectors_udf(F.col(\"embeddings\")))\n",
        "df_doc_vec_test = testData_processed.withColumn(\"doc_vector\", avg_vectors_udf(F.col(\"embeddings\")))\n",
        "\n",
        "\n",
        "def dense_vector(vec):\n",
        "\treturn Vectors.dense(vec)\n",
        "\n",
        "dense_vector_udf = F.udf(dense_vector, VectorUDT())\n",
        "training = df_doc_vec_train.withColumn(\"features\", dense_vector_udf(F.col(\"doc_vector\")))\n",
        "test = df_doc_vec_test.withColumn(\"features\", dense_vector_udf(F.col(\"doc_vector\")))\n"
      ],
      "metadata": {
        "id": "-WTipxNJkVP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sgd = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
        "sgdModelBert = sgd.fit(training)\n",
        "predictions_bert = sgdModelBert.transform(test)"
      ],
      "metadata": {
        "id": "QSGJxIaZBNi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute metrics\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "results = predictions_bert.select('text', 'label', 'prediction').toPandas()\n",
        "\n",
        "print(classification_report(results.label, results.prediction))\n",
        "print(accuracy_score(results.label, results.prediction))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waGqUkexkfHA",
        "outputId": "5cf55411-1ac7-44c4-82a7-ebeeb5441ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.80      0.78      0.79      3057\n",
            "         1.0       0.79      0.81      0.80      3159\n",
            "\n",
            "    accuracy                           0.79      6216\n",
            "   macro avg       0.79      0.79      0.79      6216\n",
            "weighted avg       0.79      0.79      0.79      6216\n",
            "\n",
            "0.7924710424710425\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.to_csv('bert_text.csv')"
      ],
      "metadata": {
        "id": "XN0bdq2Hl0SB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}